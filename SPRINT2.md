# Sprint 2: "Production Ready" (1 semana)

## Objetivo
Convertir el scaffold de Sprint 1 en un framework funcional y profesional, listo para publicar como paquete npm open-source.

## ‚ö†Ô∏è Reglas (igual que Sprint 1)
- Tests obligatorios ‚Äî cobertura ‚â• 90% (statements + branches + functions + lines)
- No se avanza sin tests de la tarea actual
- Conventional Commits
- Vector valida cada tarea
- Zero `any` impl√≠citos

---

## Estado actual (post-Sprint 1)
- ‚úÖ Memory: Redis (short-term) + PostgreSQL/pgvector (long-term) + MemoryManager
- ‚úÖ RAG: Embeddings + Weaviate vector store + HybridRAGPipeline
- ‚úÖ Reasoner: ChainOfThoughtReasoner (heur√≠sticas, no LLM real)
- ‚úÖ LLM Client: Anthropic + MiniMax (estructura correcta, endpoint de Anthropic incorrecto)
- ‚úÖ Sandbox: SafeSandbox con allowlist/blocklist
- ‚úÖ HITL: HITLManager con timeout y polling
- ‚úÖ MLOps: MLflowTracker b√°sico
- ‚úÖ CI/CD: Docker Compose + GitHub Actions + 91% coverage

## Lo que falta para "profesional"
- ‚ùå LLM client usa endpoint incorrecto (`/text/chatcompletion_v2` para Anthropic ‚Äî deber√≠a ser `/v1/messages`)
- ‚ùå Reasoner no usa LLM real en producci√≥n
- ‚ùå No hay tool system (el agente no puede hacer nada √∫til)
- ‚ùå No hay streaming
- ‚ùå Exports de npm no est√°n limpios (no hay `exports` field en package.json)
- ‚ùå No hay TypeDoc generado
- ‚ùå README es b√°sico
- ‚ùå No hay ejemplos que funcionen end-to-end sin modificaci√≥n
- ‚ùå Deuda t√©cnica: 4 archivos con branches <90%

---

## Tareas del Sprint

---

### COR-12: Fix LLM Client + Streaming (P0)
**Owner:** Codex | **Est:** 4h | **Depende:** Nada

El LLM client existe pero tiene bugs. Anthropic endpoint es `/v1/messages`, no `/text/chatcompletion_v2`. MiniMax endpoint tambi√©n necesita verificaci√≥n. Agregar streaming support.

**Subtareas:**

- [ ] **12.1** Fix Anthropic endpoint: `/v1/messages` con formato correcto
  - Request: `{ model, max_tokens, messages: [{role, content}], system?, stream? }`
  - Headers: `x-api-key`, `anthropic-version: 2023-06-01`, `content-type: application/json`
  - Response: `{ content: [{type: "text", text: "..."}], usage: {input_tokens, output_tokens} }`

- [ ] **12.2** Fix MiniMax endpoint: verificar API docs actuales
  - Si MiniMax usa OpenAI-compatible format, adaptar

- [ ] **12.3** Agregar OpenAI provider
  - Endpoint: `https://api.openai.com/v1/chat/completions`
  - Headers: `Authorization: Bearer {key}`
  - Mismo interface `LLMClient`

- [ ] **12.4** Streaming support
  - `complete()` retorna `LLMResponse` (non-streaming, como ahora)
  - `stream()` retorna `AsyncIterable<string>` (SSE parsing)
  - Anthropic: `stream: true` en body, parse SSE events
  - OpenAI: `stream: true`, parse `data: {...}` chunks

- [ ] **12.5** Retry con exponential backoff
  - Rate limits (429): retry con backoff (1s, 2s, 4s, max 3 intentos)
  - Server errors (5xx): retry 1 vez
  - Client errors (4xx excepto 429): no retry, throw

- [ ] **12.6** Tests con fetch mock (NO llama APIs reales)
  - Anthropic: request format, response parsing, error handling
  - OpenAI: request format, response parsing
  - Streaming: SSE parsing, chunk accumulation
  - Retry: 429 ‚Üí retry ‚Üí success, 500 ‚Üí retry ‚Üí fail
  - **Cobertura ‚â• 90%**

- [ ] **12.7** Commit: `fix(llm): correct API endpoints and add streaming (COR-12)`

---

### COR-13: Tool System (P0)
**Owner:** Codex | **Est:** 5h | **Depende:** COR-12

El agente necesita poder ejecutar acciones. Sin tools, es solo un chatbot. Crear sistema de tools con interface clara, registro din√°mico, y validaci√≥n con Zod.

**Subtareas:**

- [ ] **13.1** Crear `src/tools/types.ts`
  ```typescript
  interface Tool {
    name: string;
    description: string;
    parameters: ZodSchema;  // Zod schema para validaci√≥n
    execute(params: unknown): Promise<ToolResult>;
  }

  interface ToolResult {
    success: boolean;
    output: unknown;
    error?: string;
  }

  interface ToolRegistry {
    register(tool: Tool): void;
    get(name: string): Tool | undefined;
    list(): Tool[];
    execute(name: string, params: unknown): Promise<ToolResult>;
  }
  ```

- [ ] **13.2** Implementar `ToolRegistry` en `src/tools/registry.ts`
  - `register(tool)`: valida que tiene name, description, parameters, execute
  - `get(name)`: lookup por nombre
  - `list()`: retorna todas las tools registradas
  - `execute(name, params)`: valida params con Zod schema, ejecuta, retorna resultado
  - Error handling: tool no encontrada, params inv√°lidos, ejecuci√≥n falla

- [ ] **13.3** Crear built-in tools en `src/tools/builtins/`
  - **`web-search.ts`**: `{ query: string, maxResults?: number }` ‚Üí resultados de b√∫squeda (usa fetch)
  - **`calculator.ts`**: `{ expression: string }` ‚Üí resultado num√©rico (eval seguro, no `eval()`)
  - **`file-read.ts`**: `{ path: string }` ‚Üí contenido del archivo (con validaci√≥n de path)
  - **`http-request.ts`**: `{ url, method, headers?, body? }` ‚Üí response (con timeout y allowed domains)

- [ ] **13.4** Integrar tools en Agent
  - Agent tiene `withTools(registry: ToolRegistry)`
  - Reasoner detecta cu√°ndo usar tool (via LLM function calling o keyword detection)
  - Agent ejecuta tool, pasa resultado de vuelta al reasoner
  - Loop: reason ‚Üí tool ‚Üí observe ‚Üí reason (ReAct pattern)

- [ ] **13.5** Tool calling format para LLMs
  - Anthropic: usar `tools` parameter en API call
  - OpenAI: usar `functions` / `tools` parameter
  - Formato: `{ name, description, input_schema }` generado desde Zod schema

- [ ] **13.6** Tests
  - Registry: register, get, list, execute, error handling
  - Builtins: cada tool con inputs v√°lidos/inv√°lidos
  - Integration: Agent + tools + reasoner loop
  - **Cobertura ‚â• 90%**

- [ ] **13.7** Commit: `feat(tools): tool system with registry and builtins (COR-13)`

---

### COR-14: Reasoner con LLM Real (P0)
**Owner:** Codex | **Est:** 3h | **Depende:** COR-12, COR-13

Conectar el ChainOfThoughtReasoner al LLM real. El reasoner actual usa heur√≠sticas ‚Äî reemplazar con LLM para reasoning real. Integrar tool calling.

**Subtareas:**

- [ ] **14.1** Conectar Reasoner a LLMClient
  - Constructor: `{ llm: LLMClient, maxSteps?, tools?: ToolRegistry }`
  - `think()` usa `llm.complete()` en vez de `generateSimpleThought()`
  - Mantener fallback a heur√≠sticas si no hay LLM configurado

- [ ] **14.2** Mejorar prompt para ReAct pattern
  - System prompt: "You are a reasoning engine. Think step by step. Use tools when needed."
  - Format: Thought ‚Üí Action ‚Üí Observation ‚Üí Thought (loop)
  - Tool calling: generar tool calls desde LLM response, ejecutar, pasar observation

- [ ] **14.3** Structured output con JSON
  - Pedir al LLM que responda en JSON: `{ thought, action?, toolCall?, observation?, finalAnswer? }`
  - Parser robusto: manejar JSON parcial, respuestas que no siguen formato

- [ ] **14.4** Token budget management
  - Configurar max tokens por step
  - Truncar contexto si excede ventana del modelo
  - Logging de tokens usados por step

- [ ] **14.5** Tests con LLM mock
  - ReAct loop completo: think ‚Üí tool ‚Üí observe ‚Üí conclude
  - Fallback a heur√≠sticas sin LLM
  - JSON parsing: v√°lido, inv√°lido, parcial
  - Token budget exceeded
  - **Cobertura ‚â• 90%**

- [ ] **14.6** Commit: `feat(reasoning): connect reasoner to LLM with ReAct pattern (COR-14)`

---

### COR-15: npm Package Polish (P1)
**Owner:** Codex | **Est:** 3h | **Depende:** COR-12, COR-13

Preparar el paquete para publicaci√≥n en npm. Exports limpios, tipos correctos, zero dependency issues.

**Subtareas:**

- [ ] **15.1** Actualizar `package.json`
  - `name`: `cortex-ai` o `@cortex-ai/core` (verificar disponibilidad en npm)
  - `version`: `0.2.0`
  - `exports`: field con subpath exports
    ```json
    "exports": {
      ".": { "import": "./dist/index.js", "types": "./dist/index.d.ts" },
      "./memory": { "import": "./dist/memory/index.js", "types": "./dist/memory/index.d.ts" },
      "./rag": { "import": "./dist/rag/index.js", "types": "./dist/rag/index.d.ts" },
      "./tools": { "import": "./dist/tools/index.js", "types": "./dist/tools/index.d.ts" }
    }
    ```
  - `files`: solo incluir `dist/`, `README.md`, `LICENSE`
  - `engines`: `{ "node": ">=18" }`
  - `keywords`: `["ai", "agent", "llm", "rag", "memory", "tools"]`

- [ ] **15.2** Crear subpath index files
  - `src/memory/index.ts`: re-export p√∫blico
  - `src/rag/index.ts`: re-export p√∫blico
  - `src/tools/index.ts`: re-export p√∫blico
  - Solo exportar lo que el usuario necesita (no internals)

- [ ] **15.3** Verificar tree-shaking
  - Build con `tsc` ‚Üí verificar que outputs son importables individualmente
  - `import { Agent } from 'cortex-ai'` funciona
  - `import { MemoryManager } from 'cortex-ai/memory'` funciona

- [ ] **15.4** Peer dependencies
  - `ioredis`: ¬øpeer o dependency? ‚Üí peer (usuario puede no usar Redis)
  - `pg`: peer (usuario puede no usar Postgres)
  - `weaviate-client`: peer (usuario puede no usar Weaviate)
  - Solo `zod` y `uuid` como direct dependencies

- [ ] **15.5** License + .npmignore
  - MIT License file
  - `.npmignore`: tests, coverage, docker-compose, scripts, .github

- [ ] **15.6** `npm pack --dry-run` para verificar qu√© se publica
  - Solo `dist/`, `README.md`, `LICENSE`, `package.json`
  - Tama√±o < 500KB

- [ ] **15.7** Commit: `feat(npm): prepare package for npm publication (COR-15)`

---

### COR-16: README Profesional + Docs (P1)
**Owner:** Harvis (plan) + Codex (implementaci√≥n) | **Est:** 4h | **Depende:** COR-12, COR-13, COR-14

README que venda el proyecto. API docs generados. Ejemplos que funcionan.

**Subtareas:**

- [ ] **16.1** README.md completo
  - Hero section: nombre + tagline + badges (build, coverage, npm version, license)
  - Features list con emojis
  - Quick Start (5 l√≠neas para tener un agente corriendo)
  - Installation
  - Core Concepts: Memory, RAG, Reasoning, Tools, HITL
  - Configuration (todas las opciones documentadas)
  - Comparaci√≥n con competidores (tabla)
  - Contributing link
  - License

- [ ] **16.2** Ejemplos funcionales en `examples/`
  - `basic-agent.ts`: agente m√≠nimo (ya existe, actualizar)
  - `rag-chatbot.ts`: chatbot con RAG y memoria
  - `tool-agent.ts`: agente que usa tools (web search, calculator)
  - Cada ejemplo con comentarios, runnable con `npx ts-node examples/xxx.ts`

- [ ] **16.3** TypeDoc setup
  - Instalar `typedoc`
  - Config: `typedoc.json`
  - Script: `npm run docs` ‚Üí genera `docs/` con API reference
  - Deploy to GitHub Pages (opcional)

- [ ] **16.4** Architecture docs
  - `docs/architecture.md`: diagrama de componentes (Mermaid)
  - `docs/memory.md`: c√≥mo funciona el sistema de memoria
  - `docs/tools.md`: c√≥mo crear tools custom
  - `docs/reasoning.md`: ReAct pattern explicado

- [ ] **16.5** Commit: `docs: professional README and API documentation (COR-16)`

---

### COR-17: Deuda T√©cnica Sprint 1 (P1)
**Owner:** Codex | **Est:** 2h | **Depende:** Nada (paralelo)

Subir branch coverage de 4 archivos que quedaron <90%.

**Subtareas:**

- [ ] **17.1** `rag/vector-store.ts`: 71.42% ‚Üí ‚â•90%
  - Branches sin cubrir: lines 37-38 (`source || 'unknown'`, `certainty || 0`)
  - Tests: search results sin source, sin certainty

- [ ] **17.2** `memory/long-term.ts`: 86.2% ‚Üí ‚â•90%
  - Branches: lines 30,34,56-58,187,198-206
  - Tests: constructor defaults, cleanup edge cases

- [ ] **17.3** `memory/short-term.ts`: 86.66% ‚Üí ‚â•90%
  - Branch: line 32 (TTL default)
  - Test: constructor sin TTL expl√≠cito

- [ ] **17.4** `rag/embeddings.ts`: 86.66% ‚Üí ‚â•90%
  - Branches: lines 17,97  - Tests: embeddings sin API key config edge cases, batch con empty array

- [ ] **17.5** Commit: `test: resolve Sprint 1 technical debt ‚Äî all files ‚â•90% branches (COR-17)`

---

### COR-18: Error Handling + Logging (P2)
**Owner:** Codex | **Est:** 2h | **Depende:** COR-12

Errores claros para usuarios de la librer√≠a. Logging estructurado opcional.

**Subtareas:**

- [ ] **18.1** Custom error classes en `src/errors.ts`
  - `CortexError` (base)
  - `LLMError` (API failures, rate limits)
  - `MemoryError` (Redis/Postgres connection issues)
  - `ToolError` (tool execution failures)
  - `ValidationError` (invalid config, params)
  - Cada error con `code`, `message`, `cause` (para chaining)

- [ ] **18.2** Logging con nivel configurable
  - Interface `Logger { debug, info, warn, error }`
  - Default: `ConsoleLogger` (usa `console.*`)
  - Configurable: `agent.setLogger(myLogger)`
  - Structured: `{ timestamp, level, component, message, data? }`
  - Silent mode: `agent.setLogger(null)` ‚Äî zero output

- [ ] **18.3** Reemplazar todos los `throw new Error(...)` con custom errors
  - Cada m√≥dulo usa su error class
  - Mensajes descriptivos con sugerencias de fix

- [ ] **18.4** Tests: cada error class, logger levels, silent mode
- [ ] **18.5** Commit: `feat(errors): custom error classes and structured logging (COR-18)`

---

### COR-19: Multi-Agent Orchestration (P2)
**Owner:** Codex | **Est:** 4h | **Depende:** COR-12, COR-13, COR-14

Permitir que m√∫ltiples agentes colaboren. Esto diferencia a Cortex de LangChain.

**Subtareas:**

- [ ] **19.1** Crear `src/orchestrator/multi-agent.ts`
  - `AgentTeam`: grupo de agentes con roles
  - Cada agente tiene: `name`, `role`, `tools`, `instructions`
  - Orquestador decide qu√© agente habla next (round-robin, LLM-directed, o task-based)

- [ ] **19.2** Communication entre agentes
  - Shared memory: todos acceden al mismo MemoryManager
  - Message passing: agente A puede enviar mensaje a agente B
  - Context sharing: resultados de un agente son input del siguiente

- [ ] **19.3** Orchestration strategies
  - `sequential`: A ‚Üí B ‚Üí C (pipeline)
  - `parallel`: A, B, C ejecutan simult√°neamente, resultados se combinan
  - `debate`: A y B argumentan, C decide (para decisiones complejas)
  - `hierarchical`: manager asigna tasks a workers

- [ ] **19.4** Tests con mocks
  - Sequential pipeline funcional
  - Parallel execution
  - Shared memory entre agentes
  - **Cobertura ‚â• 90%**

- [ ] **19.5** Commit: `feat(orchestrator): multi-agent collaboration (COR-19)`

---

## üìä Resumen Sprint 2

| Tarea | Owner | Prio | Horas | Depende |
|-------|-------|------|-------|---------|
| COR-12 LLM Client Fix | Codex | P0 | 4h | ‚Äî |
| COR-13 Tool System | Codex | P0 | 5h | COR-12 |
| COR-14 Reasoner + LLM | Codex | P0 | 3h | COR-12, 13 |
| COR-15 npm Polish | Codex | P1 | 3h | COR-12, 13 |
| COR-16 README + Docs | Harvis+Codex | P1 | 4h | COR-12, 13, 14 |
| COR-17 Deuda T√©cnica | Codex | P1 | 2h | ‚Äî (paralelo) |
| COR-18 Errors + Logging | Codex | P2 | 2h | COR-12 |
| COR-19 Multi-Agent | Codex | P2 | 4h | COR-12, 13, 14 |

**Total: 27h**

## üìã Asignaciones

```
@Codex: COR-12 ‚Üí COR-19 (desarrollo)
@Harvis: Coordinaci√≥n + COR-16 (docs planning)
@Vector: Validaci√≥n de cada tarea
@Vega: Soporte infra si hace falta
```

## üöÄ Orden de Ejecuci√≥n

**D√≠a 1:** COR-12 (LLM fix) + COR-17 (deuda t√©cnica, paralelo)
**D√≠a 2:** COR-13 (Tools) + COR-18 (Errors, paralelo)
**D√≠a 3:** COR-14 (Reasoner + LLM real)
**D√≠a 4:** COR-15 (npm) + COR-16 (Docs)
**D√≠a 5:** COR-19 (Multi-Agent) + polish final

## üéØ Definition of Done ‚Äî Sprint 2

- [ ] `npm run build` ‚Üí 0 errores
- [ ] `npm test` ‚Üí todos pasan, coverage ‚â• 90%
- [ ] LLM client conecta a Anthropic/OpenAI reales (verificado con API key)
- [ ] Tool system funcional con ‚â•3 built-in tools
- [ ] Agent puede ejecutar tareas end-to-end (no solo scaffold)
- [ ] Ejemplos en `examples/` funcionan sin modificaci√≥n
- [ ] README profesional con Quick Start funcional
- [ ] `npm pack` produce paquete limpio <500KB
- [ ] Multi-agent orchestration funcional
- [ ] Zero `any` impl√≠citos
